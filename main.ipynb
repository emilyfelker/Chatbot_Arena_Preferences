{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f3793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from textstat import textstat\n",
    "from time import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cb6979",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "streamline code\n",
    "consider adding a few more features\n",
    "update ipython notebook for kaggle and test that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26562f7f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "start_time = time()  # to time entire program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(zip_file_path):\n",
    "    try: # for local environment\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
    "            with z.open('train.csv') as train_file:\n",
    "                train_df = pd.read_csv(train_file)\n",
    "            with z.open('test.csv') as test_file:\n",
    "                test_df = pd.read_csv(test_file)\n",
    "    except (FileNotFoundError, zipfile.BadZipFile): # for Kaggle environment\n",
    "        train_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n",
    "        test_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "    print(f\"Data loaded. Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5816f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_column(df):\n",
    "    df['target'] = df.apply(\n",
    "        lambda row: 0 if row['winner_model_a'] == 1 else (1 if row['winner_model_b'] == 1 else 2), axis=1\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae0729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bias_in_dataset(train_df, filename='bias_distribution.png'):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    blue_shade = sns.color_palette(\"Blues\")[4]\n",
    "    ax = sns.countplot(x='target', data=train_df, color=blue_shade, order=[0, 2, 1])\n",
    "    plt.title(\"Distribution of User Preferences\")\n",
    "    plt.xlabel(\"Preference\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks([0, 1, 2], ['Model A', 'Tie', 'Model B'])\n",
    "\n",
    "    # Calculate total number of rows to compute percentages\n",
    "    total = len(train_df)\n",
    "\n",
    "    # Adjust the y-limit to give more space for the percentages above the bars\n",
    "    max_height = max([p.get_height() for p in ax.patches])  # Get the max height of the bars\n",
    "    ax.set_ylim(0, max_height * 1.15)  # Increase the y-limit by 15% to avoid overlap\n",
    "\n",
    "    # Annotate percentages on the bars\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        percentage = f'{100 * height / total:.1f}%'\n",
    "        ax.annotate(percentage,\n",
    "                    (p.get_x() + p.get_width() / 2., height),\n",
    "                    ha='center', va='bottom', fontsize=12, color='black', xytext=(0, 5),\n",
    "                    textcoords='offset points')  # Move the text 5 points above the bar\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    print(f\"Bias distribution plot saved as {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e8585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(df):\n",
    "    inputs = ['prompt', 'response_a', 'response_b']\n",
    "    feature_dict = {}\n",
    "\n",
    "    # Pre-compile regex patterns\n",
    "    pronoun_patterns = {\n",
    "        'pronoun_I_count': re.compile(r'\\bI\\b', re.IGNORECASE),\n",
    "        'pronoun_you_count': re.compile(r'\\byou\\b', re.IGNORECASE),\n",
    "        'pronoun_we_count': re.compile(r'\\bwe\\b', re.IGNORECASE)\n",
    "    }\n",
    "\n",
    "    for col in inputs:\n",
    "        print(f\"Calculating features for {col}...\")\n",
    "\n",
    "        # Character Count\n",
    "        feature_dict[f'{col}_char_count'] = df[col].str.len()\n",
    "\n",
    "        # Word List and Word Count\n",
    "        word_list = df[col].str.findall(r'\\b\\w+\\b')\n",
    "        feature_dict[f'{col}_word_count'] = word_list.str.len()\n",
    "\n",
    "        # Sentence Count\n",
    "        sentence_count = df[col].str.count(r'[.!?]+').replace(0, 1)\n",
    "        feature_dict[f'{col}_sentence_count'] = sentence_count\n",
    "\n",
    "        # Average Word Length\n",
    "        feature_dict[f'{col}_avg_word_length'] = (\n",
    "            feature_dict[f'{col}_char_count'] / feature_dict[f'{col}_word_count'].replace(0, np.nan)\n",
    "        )\n",
    "\n",
    "        # Average Sentence Length (in words)\n",
    "        feature_dict[f'{col}_avg_sentence_length'] = feature_dict[f'{col}_word_count'] / sentence_count\n",
    "\n",
    "        # Punctuation Counts\n",
    "        punctuations = {\n",
    "            'exclamation_count': '!',\n",
    "            'question_count': r'\\?',\n",
    "            'comma_count': ',',\n",
    "            'period_count': r'\\.',\n",
    "            'semicolon_count': ';',\n",
    "            'colon_count': ':'\n",
    "        }\n",
    "\n",
    "        for punct_name, punct_char in punctuations.items():\n",
    "            feature_dict[f'{col}_{punct_name}'] = df[col].str.count(punct_char)\n",
    "\n",
    "        # Pronoun Counts using pre-compiled patterns\n",
    "        for pronoun_name, pattern in pronoun_patterns.items():\n",
    "            feature_dict[f'{col}_{pronoun_name}'] = df[col].str.count(pattern)\n",
    "\n",
    "        # Type-Token Ratio using vectorized operations\n",
    "        all_words = word_list.explode()\n",
    "        total_word_counts = feature_dict[f'{col}_word_count']\n",
    "        unique_word_counts = all_words.groupby(level=0).nunique()\n",
    "        feature_dict[f'{col}_type_token_ratio'] = unique_word_counts / total_word_counts.replace(0, np.nan)\n",
    "\n",
    "        # Readability Scores\n",
    "        texts = df[col].fillna('')\n",
    "        #real ones commented out for now because they are a processing bottleneck\n",
    "        feature_dict[f'{col}_flesch_reading_ease'] = texts.map(textstat.flesch_reading_ease)\n",
    "        feature_dict[f'{col}_flesch_kincaid_grade'] = texts.map(textstat.flesch_kincaid_grade)\n",
    "        # feature_dict[f'{col}_flesch_reading_ease'] = texts.map(lambda *x : 0.0)\n",
    "        # feature_dict[f'{col}_flesch_kincaid_grade'] = texts.map(lambda *x : 0.0)\n",
    "\n",
    "\n",
    "    # Convert the feature dictionary to a DataFrame and concatenate\n",
    "    feature_df = pd.DataFrame(feature_dict)\n",
    "    df = pd.concat([df.reset_index(drop=True), feature_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a7433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_differences_and_ratios(df):\n",
    "    print(\"Calculating differences and ratios between inputs...\")\n",
    "\n",
    "    pairs = [('prompt', 'response_a'), ('prompt', 'response_b'), ('response_a', 'response_b')]\n",
    "\n",
    "    basic_features = ['char_count', 'word_count', 'sentence_count', 'avg_word_length', 'avg_sentence_length',\n",
    "                      'exclamation_count', 'question_count', 'comma_count', 'period_count', 'semicolon_count',\n",
    "                      'colon_count', 'pronoun_I_count', 'pronoun_you_count', 'pronoun_we_count', 'type_token_ratio',\n",
    "                      'flesch_reading_ease', 'flesch_kincaid_grade']\n",
    "\n",
    "    diff_ratio_dict = {}\n",
    "\n",
    "    for feature in basic_features:\n",
    "        for col1, col2 in pairs:\n",
    "            diff = df[f'{col1}_{feature}'] - df[f'{col2}_{feature}']\n",
    "            ratio = df[f'{col1}_{feature}'] / df[f'{col2}_{feature}'].replace(0, np.nan)\n",
    "\n",
    "            diff_ratio_dict[f'{col1}_{col2}_{feature}_difference'] = diff\n",
    "            diff_ratio_dict[f'{col1}_{col2}_{feature}_ratio'] = ratio\n",
    "\n",
    "    diff_ratio_df = pd.DataFrame(diff_ratio_dict)\n",
    "    diff_ratio_df.index = df.index\n",
    "\n",
    "    df = pd.concat([df.reset_index(drop=True), diff_ratio_df.reset_index(drop=True)], axis=1)\n",
    "    print(\"Finished calculating differences and ratios.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbf699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_basic_features(df):\n",
    "    df = calculate_features(df)\n",
    "    df = calculate_differences_and_ratios(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765d595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_df):\n",
    "    print(\"Preparing data for training...\")\n",
    "    feature_cols = [col for col in train_df.columns if any(keyword in col for keyword in [\n",
    "        '_char_count', '_word_count', '_sentence_count',\n",
    "        '_avg_word_length', '_avg_sentence_length',\n",
    "        '_exclamation_count', '_question_count', '_comma_count', '_period_count',\n",
    "        '_semicolon_count', '_colon_count', '_pronoun_I_count', '_pronoun_you_count',\n",
    "        '_pronoun_we_count', '_type_token_ratio', '_flesch_reading_ease', '_flesch_kincaid_grade',\n",
    "        '_difference', '_ratio'\n",
    "    ])]\n",
    "\n",
    "    train_df['target'] = train_df.apply(\n",
    "        lambda row: 0 if row['winner_model_a'] == 1 else (1 if row['winner_model_b'] == 1 else 2), axis=1\n",
    "    )\n",
    "    X = train_df[feature_cols]\n",
    "    y = train_df['target']\n",
    "    X = X.fillna(0)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    print(f\"Data prepared: {X_train.shape[0]} training samples, {X_val.shape[0]} validation samples.\")\n",
    "    return X_train, X_val, y_train, y_val, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebefb216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, model_config):\n",
    "    \"\"\"\n",
    "    Train a model based on the model_config dictionary.\n",
    "\n",
    "    model_config should contain:\n",
    "      - 'type': A string specifying the type of model ('logistic_regression' or 'xgboost_rf')\n",
    "      - Other hyperparameters specific to the model\n",
    "    \"\"\"\n",
    "    print(f\"Training model: {model_config['type']}\")\n",
    "\n",
    "    if model_config['type'] == 'logistic_regression':\n",
    "        model = LogisticRegression(**model_config.get('params', {}))\n",
    "    elif model_config['type'] == 'xgboost_rf':\n",
    "        model = XGBClassifier(**model_config.get('params', {}))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_config['type']}\")\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"Model training complete: {model_config['type']}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd99fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_val, y_val, model_type=None, feature_names=None,\n",
    "                   plot_confusion=False, plot_features=False, top_n=10, filename_prefix='evaluation'):\n",
    "    print(\"Evaluating model...\")\n",
    "    y_val_pred_proba = model.predict_proba(X_val)\n",
    "    loss = log_loss(y_val, y_val_pred_proba)\n",
    "    print(f'Validation Log Loss: {loss}')\n",
    "    if plot_confusion:\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        cm = confusion_matrix(y_val, y_val_pred)\n",
    "        plot_confusion_matrix(cm, f'{filename_prefix}_confusion_matrix.png')\n",
    "        print(\"Confusion matrix plotted.\")\n",
    "    if plot_features and model_type and feature_names:\n",
    "        plot_feature_importance(model, feature_names, model_type, top_n=top_n, filename=f'{filename_prefix}_feature_importance.png')\n",
    "        print(\"Feature importance plotted.\")\n",
    "    print(\"Model evaluation complete.\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54caaaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, filename='confusion_matrix.png'):\n",
    "    reordered_indices = [0, 2, 1]  # to put the tie case in the middle\n",
    "    reordered_cm = cm[reordered_indices][:, reordered_indices]\n",
    "    cm_relative = reordered_cm.astype('float') / reordered_cm.sum(axis=1)[:, np.newaxis]\n",
    "    labels = ['Model A Wins', 'Tie', 'Model B Wins']\n",
    "    sns.heatmap(cm_relative, annot=True, fmt='.2%', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix (Relative Counts)')\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    print(f\"Confusion matrix saved as {filename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1357e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names, model_type, top_n=10, filename='feature_importance.png'):\n",
    "    if model_type == 'xgboost_rf':\n",
    "        importance = model.feature_importances_\n",
    "    elif model_type == 'logistic_regression':\n",
    "        importance = abs(model.coef_[0])\n",
    "    else:\n",
    "        raise ValueError(f\"Feature importance not implemented for model type: {model_type}\")\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    top_importance_df = importance_df.head(top_n)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Importance', y='Feature', data=top_importance_df)\n",
    "    plt.title(f'Top {top_n} Feature Importance ({model_type})')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    print(f\"Feature importance plot saved as {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab1aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, test_df, scaler):\n",
    "    print(\"Making predictions on the test set...\")\n",
    "    feature_cols = [col for col in test_df.columns if any(keyword in col for keyword in [\n",
    "        '_char_count', '_word_count', '_sentence_count',\n",
    "        '_avg_word_length', '_avg_sentence_length',\n",
    "        '_exclamation_count', '_question_count', '_comma_count', '_period_count',\n",
    "        '_semicolon_count', '_colon_count', '_pronoun_I_count', '_pronoun_you_count',\n",
    "        '_pronoun_we_count', '_type_token_ratio', '_flesch_reading_ease', '_flesch_kincaid_grade',\n",
    "        '_difference', '_ratio'\n",
    "    ])]\n",
    "    X_test = test_df[feature_cols].fillna(0)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    y_test_pred_proba = model.predict_proba(X_test_scaled)\n",
    "    submission_df = test_df[['id']].copy()\n",
    "    submission_df['winner_model_a'] = y_test_pred_proba[:, 0]\n",
    "    submission_df['winner_model_b'] = y_test_pred_proba[:, 1]\n",
    "    submission_df['winner_model_tie'] = y_test_pred_proba[:, 2]\n",
    "    print(\"Test set predictions complete.\")\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998a0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_file(submission_df, filename='submission.csv'):\n",
    "    submission_df.to_csv(filename, index=False)\n",
    "    print(f\"Submission file saved as {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba89834",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main(models_to_train):\n",
    "    train_df, test_df = load_data('data/lmsys-chatbot-arena.zip')\n",
    "    train_df = create_target_column(train_df)\n",
    "    plot_bias_in_dataset(train_df, 'bias_distribution.png')\n",
    "\n",
    "    # Add features to the train and test DataFrames\n",
    "    train_df = add_basic_features(train_df)\n",
    "    test_df = add_basic_features(test_df)\n",
    "\n",
    "    # Extract feature names after features have been added\n",
    "    feature_names = [col for col in train_df.columns if any(keyword in col for keyword in [\n",
    "        '_char_count', '_word_count', '_sentence_count',\n",
    "        '_avg_word_length', '_avg_sentence_length',\n",
    "        '_exclamation_count', '_question_count', '_comma_count', '_period_count',\n",
    "        '_semicolon_count', '_colon_count', '_pronoun_I_count', '_pronoun_you_count',\n",
    "        '_pronoun_we_count', '_type_token_ratio', '_flesch_reading_ease', '_flesch_kincaid_grade',\n",
    "        '_difference', '_ratio'\n",
    "    ])]\n",
    "\n",
    "    # Prepare the training and validation sets\n",
    "    X_train, X_val, y_train, y_val, scaler = prepare_data(train_df)\n",
    "\n",
    "    # Train and evaluate the models\n",
    "    evaluation_results = []\n",
    "    for model_config in models_to_train:\n",
    "        print(f\"Training {model_config['type']} with params: {model_config['params']}\")\n",
    "        model = train_model(X_train, y_train, model_config)\n",
    "        loss = evaluate_model(model, X_val, y_val)  # No plotting during model comparison\n",
    "        evaluation_results.append({'model': model, 'log_loss': loss, 'config': model_config})\n",
    "\n",
    "    # Identify the best model based on log loss\n",
    "    best_result = min(evaluation_results, key=lambda x: x['log_loss'])\n",
    "    best_model = best_result['model']\n",
    "    best_log_loss = best_result['log_loss']\n",
    "    best_config = best_result['config']\n",
    "\n",
    "    print(\"\\nBest Model Selected:\")\n",
    "    print(f\"Type: {best_config['type']}\")\n",
    "    print(f\"Parameters: {best_config['params']}\")\n",
    "    print(f\"Validation Log Loss: {best_log_loss}\")\n",
    "\n",
    "    # Evaluate the best model, plot the confusion matrix and feature importance\n",
    "    evaluate_model(best_model, X_val, y_val, model_type=best_config['type'],\n",
    "                   feature_names=feature_names, plot_confusion=True, plot_features=True, top_n=15)\n",
    "\n",
    "    # Make predictions using the best model\n",
    "    submission = make_predictions(best_model, test_df, scaler)\n",
    "    create_submission_file(submission)\n",
    "\n",
    "    # Print total runtime\n",
    "    print(f\"Total runtime: {time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347153c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    models_to_train = [\n",
    "        {'type': 'logistic_regression', 'params': {'solver': 'lbfgs', 'max_iter': 2000}},\n",
    "        {'type': 'xgboost_rf', 'params': {'n_estimators': 50, 'max_depth': 4, 'random_state': 42}},\n",
    "        {'type': 'xgboost_rf', 'params': {'n_estimators': 100, 'max_depth': 4, 'random_state': 42}},\n",
    "        {'type': 'xgboost_rf', 'params': {'n_estimators': 50, 'max_depth': 6, 'random_state': 42}},\n",
    "        {'type': 'xgboost_rf', 'params': {'n_estimators': 100, 'max_depth': 6, 'random_state': 42}},\n",
    "        {'type': 'xgboost_rf', 'params': {'n_estimators': 50, 'max_depth': 8, 'random_state': 42}},\n",
    "        {'type': 'xgboost_rf', 'params': {'n_estimators': 100, 'max_depth': 8, 'random_state': 42}}\n",
    "    ]\n",
    "\n",
    "    main(models_to_train)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
