{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d5efd7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f16ae0",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "add a ton of features\n",
    "unbias dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743802ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(zip_file_path):\n",
    "    try:\n",
    "        # First attempt to load data from the zip file (local environment)\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
    "            with z.open('train.csv') as train_file:\n",
    "                train_df = pd.read_csv(train_file)\n",
    "            with z.open('test.csv') as test_file:\n",
    "                test_df = pd.read_csv(test_file)\n",
    "        print(\"Data loaded from zip file.\")\n",
    "    except (FileNotFoundError, zipfile.BadZipFile):\n",
    "        # If loading from zip fails, assume we're in the Kaggle environment\n",
    "        print(\"Zip file not found or invalid. Trying to load from /input folder (Kaggle environment).\")\n",
    "        train_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n",
    "        test_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "        print(\"Data loaded from /input folder.\")\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c684ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_basic_features(df):\n",
    "    df['response_a_length'] = df['response_a'].apply(len)\n",
    "    df['response_b_length'] = df['response_b'].apply(len)\n",
    "    df['length_difference'] = df['response_a_length'] - df['response_b_length']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee591ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_df):\n",
    "    features = ['response_a_length', 'response_b_length', 'length_difference']\n",
    "\n",
    "    # Multi-class target: 0 = Model A wins, 1 = Model B wins, 2 = Tie\n",
    "    def get_target(row):\n",
    "        if row['winner_model_a'] == 1:\n",
    "            return 0  # Model A wins\n",
    "        elif row['winner_model_b'] == 1:\n",
    "            return 1  # Model B wins\n",
    "        else:\n",
    "            return 2  # Tie\n",
    "\n",
    "    train_df['target'] = train_df.apply(get_target, axis=1)\n",
    "\n",
    "    X = train_df[features]\n",
    "    y = train_df['target']\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0389338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train):\n",
    "    # Use LogisticRegression with multi-class support (using softmax under the hood)\n",
    "    model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a75883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_val, y_val):\n",
    "    # Predict probabilities for each class for the validation set\n",
    "    y_val_pred_proba = model.predict_proba(X_val)\n",
    "\n",
    "    # Compute the multi-class log loss\n",
    "    loss = log_loss(y_val, y_val_pred_proba)\n",
    "    print(f'Validation Log Loss: {loss}')\n",
    "\n",
    "    # Predict actual class labels\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_val, y_val_pred)\n",
    "    plot_confusion_matrix(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2173c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, filename='confusion_matrix.png'):\n",
    "    # Rearrange the confusion matrix to have \"tie\" in the middle\n",
    "    # Original order: [Model A Wins, Model B Wins, Tie]\n",
    "    # New order: [Model A Wins, Tie, Model B Wins]\n",
    "    reordered_cm = cm[[0, 2, 1]][:, [0, 2, 1]]  # Reorder rows and columns\n",
    "\n",
    "    # Normalize the confusion matrix by dividing each row by the sum of that row\n",
    "    cm_relative = reordered_cm.astype('float') / reordered_cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Labels for the confusion matrix\n",
    "    labels = ['Model A Wins', 'Tie', 'Model B Wins']\n",
    "\n",
    "    # Create heatmap for confusion matrix with relative counts\n",
    "    sns.heatmap(cm_relative, annot=True, fmt='.2%', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix (Relative Counts)')\n",
    "\n",
    "    # Save the plot to a file\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # Clear the plot to avoid overlapping plots in future calls\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd45bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, test_df):\n",
    "    # Add the same features to the test set\n",
    "    test_df = add_basic_features(test_df)\n",
    "\n",
    "    # Extract features for prediction\n",
    "    features = ['response_a_length', 'response_b_length', 'length_difference']\n",
    "    X_test = test_df[features]\n",
    "\n",
    "    # Predict probabilities for the test set\n",
    "    y_test_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "    # Create a submission dataframe\n",
    "    submission_df = test_df[['id']].copy()\n",
    "    submission_df['winner_model_a'] = y_test_pred_proba[:, 0]  # Probability of Model A winning\n",
    "    submission_df['winner_model_b'] = y_test_pred_proba[:, 1]  # Probability of Model B winning\n",
    "    submission_df['winner_model_tie'] = y_test_pred_proba[:, 2]  # Probability of a tie\n",
    "\n",
    "    print(submission_df.head())\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc2ab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_file(submission_df, filename='submission.csv'):\n",
    "    submission_df.to_csv(filename, index=False)\n",
    "    print(f'Submission file saved as {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0cc196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load dataset\n",
    "    train_df, test_df = load_data('data/lmsys-chatbot-arena.zip')\n",
    "\n",
    "    # Add basic features like response length\n",
    "    train_df = add_basic_features(train_df)\n",
    "    test_df = add_basic_features(test_df)\n",
    "\n",
    "    # Prepare data for training\n",
    "    X_train, X_val, y_train, y_val = prepare_data(train_df)\n",
    "\n",
    "    # Train model\n",
    "    model = train_model(X_train, y_train)\n",
    "\n",
    "    # Evaluate model\n",
    "    evaluate_model(model, X_val, y_val)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    submission = make_predictions(model, test_df)\n",
    "\n",
    "    # Create and save the submission file\n",
    "    create_submission_file(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44270b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
